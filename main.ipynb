{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot\n",
    "import seaborn\n",
    "from sklearn.feature_selection import RFE\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder paths\n",
    "liver_cancer_folder = os.path.join('.', 'liver_cancer')\n",
    "platforms_folder = os.path.join('.', 'platforms')\n",
    "\n",
    "# Dictionary to store datasets for different platforms\n",
    "platform_datasets = {}\n",
    "\n",
    "# Dictionary to store liver cancer datasets\n",
    "liver_cancer_datasets = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datasets():\n",
    "    \"\"\"\n",
    "    Process datasets from liver cancer folder and platforms folder.\n",
    "    \n",
    "    - Reads CSV files from liver_cancer_folder, converts patient status to binary, and normalizes gene expression data.\n",
    "    - Reads Excel files from platforms_folder and stores them in platform_datasets dictionary.\n",
    "    \n",
    "    Returns:\n",
    "    - liver_cancer_datasets: Dictionary containing processed liver cancer datasets.\n",
    "    - platform_datasets: Dictionary containing datasets from different platforms.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(liver_cancer_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            dataset_path = os.path.join(liver_cancer_folder, filename)\n",
    "            df = pd.read_csv(dataset_path)\n",
    "\n",
    "            # Convert patient status to binary\n",
    "            df['type'] = df['type'].apply(lambda x: 1 if x == 'HCC' else 0)\n",
    "\n",
    "            # Normalize gene expression data\n",
    "            gene_cols = df.columns.drop(['type', 'samples'])  # exclude 'samples' column\n",
    "            scaler = MinMaxScaler()\n",
    "            df[gene_cols] = scaler.fit_transform(df[gene_cols])\n",
    "            \n",
    "            liver_cancer_datasets[filename] = df\n",
    "    \n",
    "    for filename in os.listdir(platforms_folder):\n",
    "        if filename.endswith('.xlsx'):\n",
    "            dataset_path = os.path.join(platforms_folder, filename)\n",
    "            df = pd.read_excel(dataset_path)\n",
    "            platform_datasets[filename] = df\n",
    "\n",
    "    return liver_cancer_datasets, platform_datasets\n",
    "\n",
    "liver_cancer_datasets, platform_datasets = process_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" breast_cancer_platforms = {\\n    # GPL570\\t[HG-U133_Plus_2] Affymetrix Human Genome U133 Plus 2.0 Array\\n    'Breast_GSE42568.csv': 'GPL570-55999.xlsx',\\n    'Breast_GSE26910.csv': 'GPL570-55999.xlsx',\\n    'Breast_GSE45827.csv': 'GPL570-55999.xlsx',\\n    # GPL13607\\tAgilent-028004 SurePrint G3 Human GE 8x60K Microarray (Feature Number version)\\n    'Breast_GSE59246.csv': 'GPL13607-20416.xlsx',\\n    'Breast_GSE70947.csv': 'GPL13607-20416.xlsx',\\n}\""
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create lists to store datasets\n",
    "liver_cancer_platforms = {\n",
    "    # GPL571\t[HG-U133A_2] Affymetrix Human Genome U133A 2.0 Array\n",
    "    # GPL3921\t[HT_HG-U133A] Affymetrix HT Human Genome U133A Array\n",
    "    'Liver_GSE14520_U133A.csv': 'GPL571-17391.xlsx',\n",
    "    # GPL570\t[HG-U133_Plus_2] Affymetrix Human Genome U133 Plus 2.0 Array \n",
    "    'Liver_GSE62232.csv': 'GPL570-55999.xlsx'}\n",
    "\n",
    "''' breast_cancer_platforms = {\n",
    "    # GPL570\t[HG-U133_Plus_2] Affymetrix Human Genome U133 Plus 2.0 Array\n",
    "    'Breast_GSE42568.csv': 'GPL570-55999.xlsx',\n",
    "    'Breast_GSE26910.csv': 'GPL570-55999.xlsx',\n",
    "    'Breast_GSE45827.csv': 'GPL570-55999.xlsx',\n",
    "    # GPL13607\tAgilent-028004 SurePrint G3 Human GE 8x60K Microarray (Feature Number version)\n",
    "    'Breast_GSE59246.csv': 'GPL13607-20416.xlsx',\n",
    "    'Breast_GSE70947.csv': 'GPL13607-20416.xlsx',\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Liver_GSE14520_U133A.csv with GPL571-17391.xlsx\n",
      "Finding mapped genes in Liver_GSE14520_U133A.csv\n",
      "Dropped unmapped genes in Liver_GSE14520_U133A.csv\n",
      "Replacing ID's with GB_ACC's in Liver_GSE14520_U133A.csv\n",
      "Processing Liver_GSE62232.csv with GPL570-55999.xlsx\n",
      "Finding mapped genes in Liver_GSE62232.csv\n",
      "Dropped unmapped genes in Liver_GSE62232.csv\n",
      "Replacing ID's with GB_ACC's in Liver_GSE62232.csv\n"
     ]
    }
   ],
   "source": [
    "def align_datasets_with_platforms():\n",
    "    for csv_file, xlsx_file in liver_cancer_platforms.items():\n",
    "        print(f'Processing {csv_file} with {xlsx_file}')  # Add a print statement to see which files are being processed\n",
    "\n",
    "        # Get CSV dataset\n",
    "        csv_df = liver_cancer_datasets[csv_file]\n",
    "\n",
    "        # Get corresponding platform Excel dataset\n",
    "        xlsx_df = platform_datasets[xlsx_file]\n",
    "\n",
    "        # Create a mapping from 'ID' to 'GCC' in the platform dataset\n",
    "        id_to_gcc = dict(zip(xlsx_df['ID'], xlsx_df['GB_ACC']))\n",
    "\n",
    "        print(f'Finding mapped genes in {csv_file}')\n",
    "        # Find common features between dataset and platform's IDs\n",
    "        common_features = set(csv_df.columns).intersection(set(xlsx_df['ID']))\n",
    "\n",
    "        print(f'Dropped unmapped genes in {csv_file}')\n",
    "        # Drop columns from dataset that are not in platform's IDs\n",
    "        columns_to_drop = [col for col in csv_df.columns if col not in common_features]\n",
    "        csv_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "        print(f'Replacing ID\\'s with GB_ACC\\'s in {csv_file}')\n",
    "        # Replace features with 'GCC' value from platform dataset\n",
    "        for feature in common_features:\n",
    "            csv_df[feature] = csv_df[feature].map(id_to_gcc)\n",
    "\n",
    "align_datasets_with_platforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing liver_cancer datasets\n",
      "Common important features for Logistic Regression in liver_cancer datasets:\n",
      "{'PC88 <= -0.21', 'PC5 > 13.95', 'PC0 <= -41.67', 'PC0 <= -27.53', 'PC1 > 11.97', '-41.67 < PC0 <= -14.72', 'PC86 <= -1.31', 'PC2 > 14.04'}\n",
      "Common important features for Random Forest in liver_cancer datasets:\n",
      "{'PC88 <= -0.21', 'PC5 > 13.95', 'PC0 <= -41.67', 'PC0 <= -27.53', 'PC1 > 11.97', '-41.67 < PC0 <= -14.72', 'PC87 > 0.50', 'PC2 > 14.04', 'PC86 <= -1.31'}\n"
     ]
    }
   ],
   "source": [
    "# Define cancer types\n",
    "cancer_types = ['liver_cancer'] # , 'breast_cancer']\n",
    "cancer_folders = [liver_cancer_folder] # , breast_cancer_folder]\n",
    "\n",
    "# Create a dictionary to hold the classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "}\n",
    "\n",
    "# Loop through each cancer type\n",
    "for i in range(len(cancer_types)):\n",
    "\n",
    "    print(f\"Processing {cancer_types[i]} datasets\")\n",
    "\n",
    "    # Create list to store datasets\n",
    "    datasets = []\n",
    "\n",
    "    # Process datasets for current cancer type\n",
    "    for filename in os.listdir(cancer_folders[i]):\n",
    "        if filename.endswith('.csv'):\n",
    "            dataset_path = os.path.join(cancer_folders[i], filename)\n",
    "            df = pd.read_csv(dataset_path)\n",
    "            datasets.append(df)\n",
    "\n",
    "    # Store important features for each classifier\n",
    "    important_features = {\n",
    "        \"Logistic Regression\": [],\n",
    "        \"Random Forest\": [],\n",
    "    }\n",
    "\n",
    "    # Loop through each dataset\n",
    "    for df in datasets:\n",
    "\n",
    "        # Remove the first column as it's not a feature\n",
    "        df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "        # Label encoding for the target classes\n",
    "        le = LabelEncoder()\n",
    "        df['type'] = le.fit_transform(df['type'])\n",
    "\n",
    "        # Splitting into features (X) and target (y)\n",
    "        X = df.drop('type', axis=1)\n",
    "        y = df['type']\n",
    "\n",
    "        # Apply PCA for dimensionality reduction\n",
    "        n_components = min(X.shape[0], X.shape[1])\n",
    "        pca = PCA(n_components=n_components) # Adjust based on data\n",
    "        X_pca = pca.fit_transform(X)\n",
    "\n",
    "        # Splitting the dataset into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=0)\n",
    "\n",
    "        # Initialize the explainer\n",
    "        explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=[f'PC{i}' for i in range(X_train.shape[1])], class_names=le.classes_, discretize_continuous=True)\n",
    "\n",
    "        # Loop through classifiers\n",
    "        for name, clf in classifiers.items():\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            # Calculate LIME importances for a sample of instances\n",
    "            n_samples = min(200, X_train.shape[0])  # Adjust based on your data\n",
    "            sample_instances = X_train[np.random.choice(X_train.shape[0], n_samples, replace=False)]\n",
    "\n",
    "            for instance in sample_instances:\n",
    "                exp = explainer.explain_instance(instance, clf.predict_proba, num_features=X_train.shape[1])\n",
    "                importances.append(dict(exp.as_list()))\n",
    "\n",
    "            importance_df = pd.DataFrame(importances)\n",
    "\n",
    "            # Get top 10 important features\n",
    "            top_features = importance_df.mean().sort_values(ascending=False).head(10).index\n",
    "            important_features[name].append(top_features)\n",
    "\n",
    "    # ...\n",
    "\n",
    "\n",
    "    # Find and print common important features for each classifier\n",
    "    for name, features in important_features.items():\n",
    "        common_features = set(features[0]).intersection(*features)\n",
    "        print(f\"Common important features for {name} in {cancer_types[i]} datasets:\")\n",
    "        print(common_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Logistic Regression\n",
      "Train Accuracy: 0.9047619047619048\n",
      "Test Accuracy: 0.8571428571428571\n",
      "Train Confusion Matrix: \n",
      "[[55  2]\n",
      " [ 4  2]]\n",
      "Test Confusion Matrix: \n",
      "[[22  2]\n",
      " [ 2  2]]\n",
      "\n",
      "\n",
      "Classifier: Random Forest\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.9642857142857143\n",
      "Train Confusion Matrix: \n",
      "[[57  0]\n",
      " [ 0  6]]\n",
      "Test Confusion Matrix: \n",
      "[[23  1]\n",
      " [ 0  4]]\n",
      "Feature Importances: [0.41893757 0.58106243]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Additional storage for metrics and predictions\n",
    "metrics = {\n",
    "    \"Logistic Regression\": {},\n",
    "    \"Random Forest\": {},\n",
    "}\n",
    "\n",
    "# Loop through classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    # Accuracy\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Confusion matrices\n",
    "    train_conf_matrix = confusion_matrix(y_train, y_train_pred)\n",
    "    test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    # Store metrics and predictions\n",
    "    metrics[name]['train_accuracy'] = train_acc\n",
    "    metrics[name]['test_accuracy'] = test_acc\n",
    "    metrics[name]['train_confusion_matrix'] = train_conf_matrix\n",
    "    metrics[name]['test_confusion_matrix'] = test_conf_matrix\n",
    "    metrics[name]['train_predictions'] = y_train_pred\n",
    "    metrics[name]['test_predictions'] = y_test_pred\n",
    "\n",
    "    # Feature importances for Random Forest\n",
    "    if name == \"Random Forest\":\n",
    "        metrics[name]['feature_importances'] = clf.feature_importances_\n",
    "\n",
    "# Print out metrics and predictions\n",
    "for name, metric in metrics.items():\n",
    "    print(f\"Classifier: {name}\")\n",
    "    print(f\"Train Accuracy: {metric['train_accuracy']}\")\n",
    "    print(f\"Test Accuracy: {metric['test_accuracy']}\")\n",
    "    print(f\"Train Confusion Matrix: \\n{metric['train_confusion_matrix']}\")\n",
    "    print(f\"Test Confusion Matrix: \\n{metric['test_confusion_matrix']}\")\n",
    "    if name == \"Random Forest\":\n",
    "        print(f\"Feature Importances: {metric['feature_importances']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing liver_cancer datasets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\school\\terms\\spring23\\cmpe492\\Project\\main.ipynb Cell 7\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/school/terms/spring23/cmpe492/Project/main.ipynb#X36sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Apply RFE for feature selection\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/school/terms/spring23/cmpe492/Project/main.ipynb#X36sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m selector \u001b[39m=\u001b[39m RFE(clf, n_features_to_select\u001b[39m=\u001b[39mnum_features_to_select, step\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/school/terms/spring23/cmpe492/Project/main.ipynb#X36sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m selector \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/school/terms/spring23/cmpe492/Project/main.ipynb#X36sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Get the most important features\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/school/terms/spring23/cmpe492/Project/main.ipynb#X36sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m important_features_mask \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39msupport_\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:184\u001b[0m, in \u001b[0;36mRFE.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[0;32m    173\u001b[0m     \u001b[39m\"\"\"Fit the RFE model and then the underlying estimator on the selected\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m       features.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m        The target values.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:241\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    239\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFitting estimator with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m features.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m np\u001b[39m.\u001b[39msum(support_))\n\u001b[1;32m--> 241\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(X[:, features], y)\n\u001b[0;32m    243\u001b[0m \u001b[39m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    244\u001b[0m importances \u001b[39m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    245\u001b[0m     estimator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimportance_getter, transform_func\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msquare\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    246\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1406\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1405\u001b[0m     prefer \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mprocesses\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m-> 1406\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m   1407\u001b[0m                        \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(prefer\u001b[39m=\u001b[39;49mprefer))(\n\u001b[0;32m   1408\u001b[0m     path_func(X, y, pos_class\u001b[39m=\u001b[39;49mclass_, Cs\u001b[39m=\u001b[39;49m[C_],\n\u001b[0;32m   1409\u001b[0m               l1_ratio\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio, fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[0;32m   1410\u001b[0m               tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, solver\u001b[39m=\u001b[39;49msolver,\n\u001b[0;32m   1411\u001b[0m               multi_class\u001b[39m=\u001b[39;49mmulti_class, max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m   1412\u001b[0m               class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1413\u001b[0m               random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state, coef\u001b[39m=\u001b[39;49mwarm_start_coef_,\n\u001b[0;32m   1414\u001b[0m               penalty\u001b[39m=\u001b[39;49mpenalty, max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[0;32m   1415\u001b[0m               sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m   1416\u001b[0m     \u001b[39mfor\u001b[39;49;00m class_, warm_start_coef_ \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(classes_, warm_start_coef))\n\u001b[0;32m   1418\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:1041\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1033\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1041\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1042\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:859\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 859\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    860\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:777\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    776\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 777\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    778\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    779\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\fixes.py:222\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    221\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 222\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:758\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[39mif\u001b[39;00m solver \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    756\u001b[0m     iprint \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m101\u001b[39m][\n\u001b[0;32m    757\u001b[0m         np\u001b[39m.\u001b[39msearchsorted(np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]), verbose)]\n\u001b[1;32m--> 758\u001b[0m     opt_res \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[0;32m    759\u001b[0m         func, w0, method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m, jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    760\u001b[0m         args\u001b[39m=\u001b[39;49m(X, target, \u001b[39m1.\u001b[39;49m \u001b[39m/\u001b[39;49m C, sample_weight),\n\u001b[0;32m    761\u001b[0m         options\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39miprint\u001b[39;49m\u001b[39m\"\u001b[39;49m: iprint, \u001b[39m\"\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m\"\u001b[39;49m: tol, \u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_iter}\n\u001b[0;32m    762\u001b[0m     )\n\u001b[0;32m    763\u001b[0m     n_iter_i \u001b[39m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    764\u001b[0m         solver, opt_res, max_iter,\n\u001b[0;32m    765\u001b[0m         extra_warning_msg\u001b[39m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n\u001b[0;32m    766\u001b[0m     w0, loss \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\optimize\\_minimize.py:699\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    697\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    700\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[0;32m    701\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    702\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    703\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:362\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    356\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    357\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    358\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    360\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    363\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    364\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:283\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfun_and_grad\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 283\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39;49marray_equal(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx):\n\u001b[0;32m    284\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m    285\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36marray_equal\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\numeric.py:2468\u001b[0m, in \u001b[0;36marray_equal\u001b[1;34m(a1, a2, equal_nan)\u001b[0m\n\u001b[0;32m   2466\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   2467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m equal_nan:\n\u001b[1;32m-> 2468\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(asarray(a1 \u001b[39m==\u001b[39;49m a2)\u001b[39m.\u001b[39mall())\n\u001b[0;32m   2469\u001b[0m \u001b[39m# Handling NaN values if equal_nan is True\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m a1nan, a2nan \u001b[39m=\u001b[39m isnan(a1), isnan(a2)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a dictionary to hold the classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "}\n",
    "\n",
    "# Number of features to select\n",
    "num_features_to_select = 50  # Adjust based on your preference\n",
    "\n",
    "# Loop through each cancer type\n",
    "for i in range(len(cancer_types)):\n",
    "\n",
    "    print(f\"Processing {cancer_types[i]} datasets\")\n",
    "\n",
    "    # Create list to store datasets\n",
    "    datasets = []\n",
    "\n",
    "    # Process datasets for current cancer type\n",
    "    for filename in os.listdir(cancer_folders[i]):\n",
    "        if filename.endswith('.csv'):\n",
    "            dataset_path = os.path.join(cancer_folders[i], filename)\n",
    "            df = pd.read_csv(dataset_path)\n",
    "            datasets.append(df)\n",
    "\n",
    "    # Store important features for each classifier\n",
    "    important_features = {\n",
    "        \"Logistic Regression\": [],\n",
    "        \"Random Forest\": [],\n",
    "    }\n",
    "\n",
    "    # Loop through each dataset\n",
    "    for df in datasets:\n",
    "\n",
    "        # Remove the first column as it's not a feature\n",
    "        df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "        # Label encoding for the target classes\n",
    "        le = LabelEncoder()\n",
    "        df['type'] = le.fit_transform(df['type'])\n",
    "\n",
    "        # Splitting into features (X) and target (y)\n",
    "        X = df.drop('type', axis=1)\n",
    "        y = df['type']\n",
    "\n",
    "        # Splitting the dataset into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "        # Loop through classifiers\n",
    "        for name, clf in classifiers.items():\n",
    "            # Fit the classifier\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            # Apply RFE for feature selection\n",
    "            selector = RFE(clf, n_features_to_select=num_features_to_select, step=1)\n",
    "            selector = selector.fit(X_train, y_train)\n",
    "\n",
    "            # Get the most important features\n",
    "            important_features_mask = selector.support_\n",
    "            important_features[name] = [feature for feature, selected in zip(X_train.columns, important_features_mask) if selected]\n",
    "\n",
    "    # Find and print common important features for each classifier\n",
    "    for name, features in important_features.items():\n",
    "        print(f\"Important features for {name} in {cancer_types[i]} datasets:\")\n",
    "        print(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
